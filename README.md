# hcd_asmt_2

#reflection

A toxic comment is defined as a rude, disrespectful, or unreasonable comment that is likely to make other users leave a discussion. I learned that most comments that contain common bad words such as "fuck", "shit" are likely to considered as "toxic", except when those are abbreviated. For example, toxicity score of "wtf" was 0.5535644, while the toxicity score of "what the fuck" was 0.8742253.
Biases obviously exist. Even though comments don't contain bad words, they still could be toxic and abusive. At the same time, comments that contain bad words could be expressed in a fun/positive way, which people typically don't consider them as toxic and abusive.
One thing that came up to my mind was that whole dataset for this assignment could be biased as well, since perspectives on determining whether the comment is toxic or not differ by all people.
To test my null hypothesis, I tried to get t test and p value (which I don't think it's accurate), and I think biases exist in this test as well. Which determines if the comment is toxic and abusive? I saw the limitation to set the line to consider the comment as toxic.
